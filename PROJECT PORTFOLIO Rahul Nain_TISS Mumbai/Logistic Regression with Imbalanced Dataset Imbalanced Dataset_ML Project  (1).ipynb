{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3689760c-41f8-4a33-9c96-3fd17803950e",
    "_uuid": "3e0ad409d438c7c68ea6a76700a1e964a357453f"
   },
   "source": [
    "<h1 align=\"center\"> Logistic Regression On IMBALANCED DATASET </h1>\n",
    "\n",
    "# Gather Sense of Our Data:\n",
    "The initial step necessitates the acquisition of a rudimentary comprehension of our dataset. It is imperative to acknowledge that, with the exception of the 'transaction' and 'amount' attributes, the nature of the remaining columns remains undisclosed, primarily ascribed to privacy considerations. However, it is worth noting that the unrevealed attributes have already undergone a process of standardization.\n",
    "\n",
    "\n",
    "# Summary: \n",
    "The transaction amount is relatively small. \n",
    "The mean of all the mounts made is approximately USD 88.\n",
    "There are no \"Null\" values, so we don't have to work on ways to replace values.\n",
    "Most of the transactions were Non-Fraud (99.83%) of the time, while Fraud transactions occurs (017%) of the time in the dataframe.\n",
    "\n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "We will start with importing the important libraries and then working to get the basic insights from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Imported Libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/Rahul Nain/OneDrive/Desktop/MSc Analytics/SEM 3/ML/creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "376ce881-463a-4a09-9ac0-c63f85577eec",
    "_kg_hide-input": true,
    "_uuid": "93031e732e5aca3a2b4984799d6bf58d76e4b52d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.918649e-15</td>\n",
       "      <td>5.682686e-16</td>\n",
       "      <td>-8.761736e-15</td>\n",
       "      <td>2.811118e-15</td>\n",
       "      <td>-1.552103e-15</td>\n",
       "      <td>2.040130e-15</td>\n",
       "      <td>-1.698953e-15</td>\n",
       "      <td>-1.893285e-16</td>\n",
       "      <td>-3.147640e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.473120e-16</td>\n",
       "      <td>8.042109e-16</td>\n",
       "      <td>5.282512e-16</td>\n",
       "      <td>4.456271e-15</td>\n",
       "      <td>1.426896e-15</td>\n",
       "      <td>1.701640e-15</td>\n",
       "      <td>-3.662252e-16</td>\n",
       "      <td>-1.217809e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.918649e-15  5.682686e-16 -8.761736e-15  2.811118e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552103e-15  2.040130e-15 -1.698953e-15 -1.893285e-16 -3.147640e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.473120e-16  8.042109e-16  5.282512e-16  4.456271e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.426896e-15  1.701640e-15 -3.662252e-16 -1.217809e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "03ddb929-5bc8-4af4-90cd-21dcbb57560d",
    "_kg_hide-input": true,
    "_uuid": "38bec67888aa534e9739e95ef9fac62d27a87021"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good No Null Values!\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "6a526b6c-8463-4f6f-92b0-e8a3a21cbb2e",
    "_kg_hide-input": true,
    "_uuid": "479a5f12d3dd68262316a17b4b7b3499e0a2cbe0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "01c007fa-0fcc-4eea-84ff-0861a2f8c533",
    "_kg_hide-input": true,
    "_uuid": "f6b96ff34855e3bf7af1f6979342b01c473e4e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Dataset:\n",
    "The above analysis completely indicates that the dataset is highly imbalanced. \n",
    "The Frauds are only 0.17% of the total data values. This indicates that the statistial models that we will be applying might give us biased results or unreliable if not skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "657bc987-4b15-4cfa-b290-c39a2632e2ac",
    "_kg_hide-input": true,
    "_uuid": "337caaf6ed3f65beedb24a74eebb22d97ff52ba4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class Distributions \\n (0: No Fraud || 1: Fraud)')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEoCAYAAABl8ecgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAf60lEQVR4nO3de5xcRZ338c8XAggoCCTcEkJYiS5BV5QxsoqK8giou8tlQYKuRI2GF4IL3sFVYWFReERRQFCQEGCVywMicTUbwkURjcAEIwmJQIAYAjEJJEIQARN+zx9VLWc6PTM9k6nuYfJ9v179mu46VTV1ZpL5dp1z+pQiAjMzs4G2UbsHYGZmQ5MDxszMinDAmJlZEQ4YMzMrwgFjZmZFOGDMzKwIB4y95ElaJOmz7R5HbySNkRSSOgr0faqkeZXXUyX9z0B/n9x3sf2wocUBY4OapB0kfVvSg5Kek/SopOmS3tvusdXkP7a1xzOSHpL0Q0n71lV9BNgJmNNkv30JzrOBdzQ/6uZI+rmk8+uK+7QftuFywNigJWkMcDdwIHAy8A/A/wF+Cny3fSNr6OOkP7p7AJOA54HbJH2uViEi1kbEHyNizUB9U0kbSdo4Ip6OiCcGqt+elNgPG5ocMDaYXQAI6IiIayLivohYEBHnA6/vrpGkT0u6R9Kf84zn+5JeWdm+taQrJC2X9GyecZxY2X6MpPvzthWSZkga1stY/5T/6P4hIm6NiA8DZwJfk7R77rfLoSVJm0g6V9JjeXb2iKQz87afA7sCX6/NjnL5hyU9Lem9+ZDY88Ae9YfIKvvyJUnLcptLJW1e2bbO7KR6aE3SVNKs6LjKDG1Mo0Nkkt4u6Y78M1sm6RxJm9Z9rwskfVXS4/lnf7akjSp1Dsu/t79IWinpF5J26OXnboOYA8YGJUnbAgcB50fE0/XbI2JVD81fAE4E9gQ+AIwHzqts/y/gdcA/AX8PfBR4NH/fDuA7wH8CryHNmP63n7vxDdL/sUO62f7vwKHABGAscCRwX952GLAEOI00M9qp0u5lwJeAY4BxwB+66f8dpCDeH/hX4ADgrD6M/wRgFnBpZQyP1FeSNBKYDvwWeANpBncU8LW6qh8E1gBvAY4n/Y6OzH3sCFwFXEaaBb4duKIPY7VBqLd3ZWbtsjtp9rKgrw0j4luVl4skfR64QdLEiHiBNDP4bUTcWatTqT8a+DMwLSJWk/54/67vw4eIeELScuDvuqmyK3A/8MtINwVcDPw6t10paS2wOiL+WNduY+CTETG7ViCpUf9rgY/kgJ4n6QvAJZJOjog/NzH+JyU9DzxTHUOD7/UJYCnwifzzXSDpJOB7kr4cEc/kevMj4iv5+f2SPk4KvyuBnYFNgGsjohaY68zI7KXFMxgbrBr+xWyqofQuSTMlLZG0GvgRsCmwY65yIfB+Sb/Lh2mqJ8dnkkLlYUk/kDRR0iv6OxbSfnR3R9mpwF6kP7bfkfS+6iGjHqyhuRPs99TN/maRfg6vaqJtX+wBzMrhUnN7/l67V8dT1+4xYPv8/HfATaQgvE7SsZJGDPA4rcUcMDZYPUD6w7xHXxpJ2pV0EcAC4Ahgb9IhMEh/8IiI6aTZw9nAcOCnki7N21YDbwTeT5pRnAz8XtLOfd0BScOBEcBDjbZHxN3AGOCLpP+LlwEzmwiZ5yJibV/H08ALrBvkm/Sjn55CtFr+1wbbNoJ04QDpEN4BpCCaBDwgqdtzbTb4OWBsUIqIlcAM4HhJL6/fXj1pX6eDFCSfiohZEXE/6fBLff+PR8QV+WT8JGCipM3ytjURcUtE1K5c25J0vqavPkP6I35DdxUiYnVE/L+IOBZ4H/AuXnzX/zzpcFh/vU7SlpXX++Q+H8yvV9D13A6se/FEM2OYD/xjXTDuW/e9ehXJrIj4T+BNpBnOkc22t8HH52BsMPsE6ZxEp6Qvk97ZCngnaWYxukGbB0hvnE6U9CPSH9UTqxUknUa6/Ple0v+Bw4CHIuI5Sf9EOoR0G7Ayf69X0Pu5oFfmE9W1Q1ATgaOBz0fEwkYNJH2adO5iDund/QeAp0gn9yGdG3qbpP8mzVoe72UM9YYBU/L+7ky6qu3iyvmXW4BvSfoX0sUFxwC70PWc1CJgvNIl40+Tfib1LiD9jC+Q9G3SOaczSRdoPNOg/jok7UO6oGIGsIx0scAupPCylygHjA1aEfGwpDeSDiGdBYwEniAdrz+mmzb3SDoB+ALparFfA58Frq5Uew44A9gNeBb4DfDPedufSFd9fQXYgvQO/GMR8ctehntxpe+luc/9IuK2HtqsBj5HuoIsSFdhvafyR/krwPfyGDaj7+elfkEK0VvzvlwHfL6yfQpphjYlv74AuJ502LDmbNKhu/nA5qSfWRcR8aik9wBfJ4Xln4Afkn5vzXoSeCvwSeCVpKvVTo+I/+5DHzbIyCtamplZCT4HY2ZmRThgzMysCAeMmZkV4YAxM7MiHDBmZlaEA8ZaJt/xd0rvNa07kv4n3+W4pzqLJO1Xeb2fpEVlRzb4SBqe7/q8X379OqW7a2/Zc0sbKA4YawlJ2wOfJn02pVr+CUkP59u8z5b0tn70fWr+Q/L9uvIBWXlRXRcUqz3mrE+fg42kPSVdq7R0QUg6tZ/9fLibn9eJAzvivouIuaTPJ3263WPZUDhgrFU+BtwZEX+7L5ekI4FvA18lfXL718B0SY0+od+bZ4EPS9pzIAbbQG1Bsdpj/0aVJPXnXl6DwRakT+1/CXh4Pft6hq4/q52Ai+orSRqmbm4DXdClwLHqfX0fGwAOGGuVDwDT6so+DUyNiIvzQmKfJH0K/th+9P8g6TYj9WuQdJEPk9xUWdRqqqStm+i/tqBY7fFEZYZ0lKRbJP0FOEbSdpKuzHdz/oukeyV9pG4cPS72lV9vkcueVlrEqy+fjO+TiLgrIj4bET8kBcR6dtflZ/XHiHgmzzTn5VnOg6S7Hmwp6SBJv5S0Kv9OZkj6201Ou5uJ5rLDK6/flGfBz0r6LfDmBmO7EdgW2G8999Ga4ICx4pQWDxsHdFbKNiXd6fjGuuo3khakqtU7VXk1xyacBLyvu8NskrYgLR72NGkRskPz91rf80JfI91mZRzwY9KCYHeTbpC5J2mW9j1JDWc9PTgbeDdpsbD9SbO8t6/nWPutcvhrzHp0sxvpzcYRpBtrPku6mei3SL+T/Ui3jfmJKitiNjG2LUl30X6IdMPTk0g/vy4i4nnS7WzeUb/NBp6nidYKo0n30VpaKRtOukvvsrq6y0g3Pax5nBdXeexRRMyVdDnwf4F/bFDlg8DLgQ/l2/IjaTJwq6Tdu7spZXZF3cn1Y4Bf5efnRcS1dfW/Xnl+kaR3kVZ5vLmZfVG6g/Qk4KMRMSOXfYQXb4TZDk+Sfhf1t92vt6WkLquQRkTtjtibkn7+1d/7ddW6eT+fIgXO7U2O7YO57+oCa2fQeFXMx0jLJFhhDhhrhdo68M822FY/O+mytkhEnA+cT/O+QlrA6zDSLKJqD9IiXKsrZb8m3VJ/HNBTwHyOrksnLwO2y887qxUlbUx6B30k6Qadm5H++P28D/vxqtxmVq0gIp6WNLcPfQyoiLiedDPM3jxDWkitkSV14YKkVwGnkw5pjSAdWdmIxnfL7k7td1u/wFojf+HFf5NWkAPGWqF2m/lteHEW8zhpSd8d6+puz7qzmqZFxCOSziMdtnpf3eZmF8Zq5I/1MxxJtYCpX374s6S1YE4A5pIOyX2VF1dvhN4X+2r1ye+BFD3MBhst1fwT4FHSrPBR0oqd88kLxJF+VlD5mTS4mKIvP69t6bokgRXiczDWCg+SDnmMqxXkY+GzSecYqt5NXpd+PXyN9E74Y3Xl84HXq+sSyG8h/T/obb2XvtgX+Ele0GwOaf9fXVent8W+FpIORe1TK8jnGV47gONsuxzSewBfjYibImIBaf2d6pvfFflr9ee1V11X82m8wFojr2Xd2a0V4ICx4vJa7TeR/vBWfZN0afHHJO2htFjVzsB3axUkHS/p9338fqtIM4YT6jb9gPQO+vJ8NdnbSeut/KiX8y99dT+wv6R9Jf096RBf/ToqtwDvkfQvkl4j6ZukBbZq+/A0cAlwlqR358uvp7B+K1x2S9KmkvaStBfpIoUd8+vdK3UOlfR7SSMH8FuvIs1mPy5pd0nvIP3+19QqRMRfSJ9f+UL+vM5bWPcE/g9zmym5zruB/2iwn2NIhy3rLy6xAhww1ioXAUfm8xMARMTVpJUQv0S6smdf4L0R8YdKu+HAa/rx/c4DllcL8kJeBwJbAXeSljKeBXy0H/335L9y/9NJK2P+mRRuVVMqj1+RDqPVn9/4LGmxsOvz13m5vxJ2Ji149lvS+Z9j8vPqh1e3Jv0uBuyzPvnNx5Gkhc/mAd8Bvky6hLmq9ju6i/Sm4Et1/TxNumpvLGl2cjZp0bl6RwE31v0bs0K84Ji1jKRZwAUR0ejKHhsgSreF+XBE/Dy/3o/0eaMxbRvUICBpM9KS2kdFxK96q2/rzzMYa6Vj8L85a59dgTMcLq3jq8isZSLiHuCedo/DNkwRcT/p/Ji1iN9Nmg0936LrZbiLcplZS/kcjJmZFeFDZNnw4cNjzJgx7R6GmdlLyuzZsx+PiBGNtjlgsjFjxtDZ2dl7RTMz+xtJ3V7y7XMwZmZWhAPGzMyKcMCYmVkRDhgzMyvCAWNmZkU4YMzMrAgHjJmZFeGAMTOzIhwwZmZWhD/JP4BGj7693UOwQWjx4vqFPM02DJ7BmJlZEQ4YMzMrwgFjZmZFOGDMzKwIB4yZmRXhgDEzsyIcMGZmVoQDxszMinDAmJlZEQ4YMzMrwgFjZmZFOGDMzKwIB4yZmRXhgDEzsyIcMGZmVoQDxszMinDAmJlZEQ4YMzMrwgFjZmZFOGDMzKyIYgEjaRdJt0paIOleSSfk8lMlPSppTn68t9LmZEkLJd0n6cBK+d6S5uZt50pSLt9M0tW5/A5JYyptJkp6ID8mltpPMzNrbFjBvtcAn4mIuyW9ApgtaWbedk5EnF2tLGkcMAHYE9gZuEnSqyNiLXAhMBn4DfAz4CBgOjAJWBURu0uaAJwFHClpW+AUoAOI/L2nRcSqgvtrZmYVxWYwEbE0Iu7Oz1cDC4CRPTQ5GLgqIp6LiIeBhcB4STsBW0XErIgI4HLgkEqby/Lza4H98+zmQGBmRKzMoTKTFEpmZtYiLTkHkw9dvQG4IxcdL+keSVMkbZPLRgKPVJotyWUj8/P68i5tImIN8CSwXQ99mZlZixQPGEkvB64DToyIp0iHu14F7AUsBb5Rq9qgefRQ3t821bFNltQpqXPFihU97YaZmfVR0YCRtAkpXH4QET8CiIhlEbE2Il4ALgbG5+pLgF0qzUcBj+XyUQ3Ku7SRNAzYGljZQ19dRMRFEdERER0jRoxYn101M7M6Ja8iE3AJsCAivlkp36lS7VBgXn4+DZiQrwzbDRgL3BkRS4HVkvbJfR4N3FBpU7tC7HDglnyeZgZwgKRt8iG4A3KZmZm1SMmryN4KfAiYK2lOLvsicJSkvUiHrBYBxwBExL2SrgHmk65AOy5fQQZwLDAV2Jx09dj0XH4JcIWkhaSZy4Tc10pJpwN35XqnRcTKIntpZmYNKb3ht46Ojujs7FyvPkaPvn2ARmNDyeLF+7Z7CGbFSJodER2NtvmT/GZmVoQDxszMinDAmJlZEQ4YMzMrwgFjZmZFOGDMzKwIB4yZmRXhgDEzsyIcMGZmVoQDxszMinDAmJlZEQ4YMzMrwgFjZmZFOGDMzKwIB4yZmRXhgDEzsyIcMGZmVoQDxszMinDAmJlZEQ4YMzMrwgFjZmZFOGDMzKwIB4yZmRXhgDEzsyIcMGZmVoQDxszMinDAmJlZEcUCRtIukm6VtEDSvZJOyOXbSpop6YH8dZtKm5MlLZR0n6QDK+V7S5qbt50rSbl8M0lX5/I7JI2ptJmYv8cDkiaW2k8zM2us5AxmDfCZiNgD2Ac4TtI44CTg5ogYC9ycX5O3TQD2BA4CLpC0ce7rQmAyMDY/Dsrlk4BVEbE7cA5wVu5rW+AU4M3AeOCUapCZmVl5xQImIpZGxN35+WpgATASOBi4LFe7DDgkPz8YuCoinouIh4GFwHhJOwFbRcSsiAjg8ro2tb6uBfbPs5sDgZkRsTIiVgEzeTGUzMysBVpyDiYfunoDcAewQ0QshRRCwPa52kjgkUqzJblsZH5eX96lTUSsAZ4EtuuhLzMza5HiASPp5cB1wIkR8VRPVRuURQ/l/W1THdtkSZ2SOlesWNHD0MzMrK+KBoykTUjh8oOI+FEuXpYPe5G/Ls/lS4BdKs1HAY/l8lENyru0kTQM2BpY2UNfXUTERRHREREdI0aM6O9umplZAyWvIhNwCbAgIr5Z2TQNqF3VNRG4oVI+IV8ZthvpZP6d+TDaakn75D6PrmtT6+tw4JZ8nmYGcICkbfLJ/QNymZmZtciwgn2/FfgQMFfSnFz2ReBM4BpJk4DFwBEAEXGvpGuA+aQr0I6LiLW53bHAVGBzYHp+QAqwKyQtJM1cJuS+Vko6Hbgr1zstIlYW2k8zM2tA6Q2/dXR0RGdn53r1MXr07QM0GhtKFi/et91DMCtG0uyI6Gi0zZ/kNzOzIhwwZmZWhAPGzMyKcMCYmVkRDhgzMyvCAWNmZkU4YMzMrAgHjJmZFeGAMTOzIhwwZmZWhAPGzMyKcMCYmVkRDhgzMyvCAWNmZkU4YMzMrAgHjJmZFeGAMTOzIhwwZmZWhAPGzMyKcMCYmVkRTQWMpJubKTMzM6sZ1tNGSS8DtgCGS9oGUN60FbBz4bGZmdlLWI8BAxwDnEgKk9m8GDBPAd8pNywzM3up6zFgIuLbwLclfTIizmvRmMzMbAjobQYDQEScJ+ktwJhqm4i4vNC4zMzsJa6pgJF0BfAqYA6wNhcH4IAxM7OGmgoYoAMYFxFRcjBmZjZ0NPs5mHnAjiUHYmZmQ0uzATMcmC9phqRptUdPDSRNkbRc0rxK2amSHpU0Jz/eW9l2sqSFku6TdGClfG9Jc/O2cyUpl28m6epcfoekMZU2EyU9kB8Tm9xHMzMbQM0eIju1H31PBc5n3fM050TE2dUCSeOACcCepEuib5L06ohYC1wITAZ+A/wMOAiYDkwCVkXE7pImAGcBR0raFjiFdFgvgNmSpkXEqn7sg5mZ9VOzV5H9oq8dR8Rt1VlFLw4GroqI54CHJS0ExktaBGwVEbMAJF0OHEIKmIN5MfiuBc7Ps5sDgZkRsTK3mUkKpSv7ug9mZtZ/zd4qZrWkp/LjWUlrJT3Vz+95vKR78iG0bXLZSOCRSp0luWxkfl5f3qVNRKwBngS266EvMzNroaYCJiJeERFb5cfLgH8lHf7qqwtJlzvvBSwFvpHL1aBu9FDe3zZdSJosqVNS54oVK3oYtpmZ9VW/7qYcET8G3tWPdssiYm1EvABcDIzPm5YAu1SqjgIey+WjGpR3aSNpGLA1sLKHvhqN56KI6IiIjhEjRvR1d8zMrAfNHiI7rPI4XNKZdDMr6KWfnSovDyVd/gwwDZiQrwzbDRgL3BkRS4HVkvbJ51eOBm6otKldIXY4cEv+nM4M4ABJ2+RDcAfkMjMza6FmryL758rzNcAi0kn2bkm6EtiPdCfmJaQru/aTtBcpnBaRbqZJRNwr6Rpgfu7/uHwFGcCxpCvSNied3J+eyy8BrsgXBKwkXYVGRKyUdDpwV653Wu2Ev5mZtY784fyko6MjOjs716uP0aNvH6DR2FCyePG+7R6CWTGSZkdER6NtzR4iGyXp+vzByWWSrpM0qveWZma2oWr2JP+lpHMeO5Mu+f1JLjMzM2uo2YAZERGXRsSa/JgK+LIrMzPrVrMB87ikf5O0cX78G/BEyYGZmdlLW7MB81Hg/cAfSR+QPBz4SKlBmZnZS1+zlymfDkys3TAy31DybFLwmJmZraPZGcw/VO9GnD9X8oYyQzIzs6Gg2YDZqHJjytoMptnZj5mZbYCaDYlvAL+WdC3pU/jvB84oNiozM3vJa3Y9mMsldZJucCngsIiYX3RkZmb2ktb0Ya4cKA4VMzNrSr9u129mZtYbB4yZmRXhgDEzsyIcMGZmVoQDxszMinDAmJlZEQ4YMzMrwgFjZmZFOGDMzKwIB4yZmRXhgDEzsyIcMGZmVoQDxszMinDAmJlZEQ4YMzMrwgFjZmZFFAsYSVMkLZc0r1K2raSZkh7IX7epbDtZ0kJJ90k6sFK+t6S5edu5kpTLN5N0dS6/Q9KYSpuJ+Xs8IGliqX00M7PulZzBTAUOqis7Cbg5IsYCN+fXSBoHTAD2zG0ukLRxbnMhMBkYmx+1PicBqyJid+Ac4Kzc17bAKcCbgfHAKdUgMzOz1igWMBFxG7Cyrvhg4LL8/DLgkEr5VRHxXEQ8DCwExkvaCdgqImZFRACX17Wp9XUtsH+e3RwIzIyIlRGxCpjJukFnZmaFtfoczA4RsRQgf90+l48EHqnUW5LLRubn9eVd2kTEGuBJYLse+jIzsxYaLCf51aAseijvb5uu31SaLKlTUueKFSuaGqiZmTWn1QGzLB/2In9dnsuXALtU6o0CHsvloxqUd2kjaRiwNemQXHd9rSMiLoqIjojoGDFixHrslpmZ1Wt1wEwDald1TQRuqJRPyFeG7UY6mX9nPoy2WtI++fzK0XVtan0dDtySz9PMAA6QtE0+uX9ALjMzsxYaVqpjSVcC+wHDJS0hXdl1JnCNpEnAYuAIgIi4V9I1wHxgDXBcRKzNXR1LuiJtc2B6fgBcAlwhaSFp5jIh97VS0unAXbneaRFRf7GBmZkVpvSm3zo6OqKzs3O9+hg9+vYBGo0NJYsX79vuIZgVI2l2RHQ02jZYTvKbmdkQ44AxM7MiHDBmZlaEA8bMzIpwwJiZWREOGDMzK8IBY2ZmRThgzMysCAeMmZkV4YAxM7MiHDBmZlaEA8bMzIpwwJiZWREOGDMzK8IBY2ZmRThgzMysCAeMmZkV4YAxM7MiHDBmZlaEA8bMzIpwwJiZWREOGDMzK8IBY2ZmRThgzMysCAeMmZkV4YAxM7MiHDBmZlZEWwJG0iJJcyXNkdSZy7aVNFPSA/nrNpX6J0taKOk+SQdWyvfO/SyUdK4k5fLNJF2dy++QNKblO2lmtoFr5wzmnRGxV0R05NcnATdHxFjg5vwaSeOACcCewEHABZI2zm0uBCYDY/PjoFw+CVgVEbsD5wBntWB/zMysYjAdIjsYuCw/vww4pFJ+VUQ8FxEPAwuB8ZJ2AraKiFkREcDldW1qfV0L7F+b3ZiZWWu0K2ACuFHSbEmTc9kOEbEUIH/dPpePBB6ptF2Sy0bm5/XlXdpExBrgSWC7AvthZmbdGNam7/vWiHhM0vbATEm/76Fuo5lH9FDeU5uuHadwmwwwevTonkdsZmZ90pYZTEQ8lr8uB64HxgPL8mEv8tflufoSYJdK81HAY7l8VIPyLm0kDQO2BlY2GMdFEdERER0jRowYmJ0zMzOgDQEjaUtJr6g9Bw4A5gHTgIm52kTghvx8GjAhXxm2G+lk/p35MNpqSfvk8ytH17Wp9XU4cEs+T2NmZi3SjkNkOwDX53Puw4AfRsT/SroLuEbSJGAxcARARNwr6RpgPrAGOC4i1ua+jgWmApsD0/MD4BLgCkkLSTOXCa3YMTMze1HLAyYiHgJe36D8CWD/btqcAZzRoLwTeG2D8mfJAWVmZu0xmC5TNjOzIcQBY2ZmRThgzMysCAeMmZkV4YAxM7MiHDBmZlaEA8bMzIpwwJiZWREOGDMzK8IBY2ZmRThgzMysCAeMmZkV4YAxM7MiHDBmZlaEA8bMzIpwwJiZWREOGDMzK8IBY2ZmRThgzMysCAeMmZkV4YAxM7MiHDBmZlaEA8bMzIpwwJiZWREOGDMzK8IBY2ZmRThgzMysCAeMmZkVMaQDRtJBku6TtFDSSe0ej5nZhmTIBoykjYHvAO8BxgFHSRrX3lGZmW04hmzAAOOBhRHxUEQ8D1wFHNzmMZmZbTCGtXsABY0EHqm8XgK8uU1jMWu720ePbvcQbBDad/HiYn0P5YBRg7LoUkGaDEzOL5+WdF/xUW04hgOPt3sQg4Ea/Uu0dvO/z5r1/we6a3cbhnLALAF2qbweBTxWrRARFwEXtXJQGwpJnRHR0e5xmDXif5+tMZTPwdwFjJW0m6RNgQnAtDaPycxsgzFkZzARsUbS8cAMYGNgSkTc2+ZhmZltMIZswABExM+An7V7HBsoH3q0wcz/PltAEdF7LTMzsz4ayudgzMysjRwwNuB8ix4bjCRNkbRc0rx2j2VD4YCxAeVb9NggNhU4qN2D2JA4YGyg+RY9NihFxG3AynaPY0PigLGB1ugWPSPbNBYzayMHjA20Xm/RY2YbBgeMDbReb9FjZhsGB4wNNN+ix8wAB4wNsIhYA9Ru0bMAuMa36LHBQNKVwCzgNZKWSJrU7jENdf4kv5mZFeEZjJmZFeGAMTOzIhwwZmZWhAPGzMyKcMCYmVkRDhizNpC0o6SrJD0oab6kn0l6te/0a0PJkF7R0mwwkiTgeuCyiJiQy/YCdmjnuMwGmmcwZq33TuCvEfHdWkFEzKFyk1BJYyT9UtLd+fGWXL6TpNskzZE0T9LbJG0saWp+PVfSp1q+R2YNeAZj1nqvBWb3Umc58O6IeFbSWOBKoAP4ADAjIs7Ia+9sAewFjIyI1wJIemWpgZv1hQPGbHDaBDg/HzpbC7w6l98FTJG0CfDjiJgj6SHg7ySdB/wUuLEdAzar50NkZq13L7B3L3U+BSwDXk+auWwKf1s06+3Ao8AVko6OiFW53s+B44Dvlxm2Wd84YMxa7xZgM0kfrxVIehOwa6XO1sDSiHgB+BCwca63K7A8Ii4GLgHeKGk4sFFEXAd8GXhja3bDrGc+RGbWYhERkg4FviXpJOBZYBFwYqXaBcB1ko4AbgX+nMv3Az4n6a/A08DRpBVDL5VUe8N4cul9MGuG76ZsZmZF+BCZmZkV4YAxM7MiHDBmZlaEA8bMzIpwwJiZWREOGDMzK8IBY2ZmRThgzMysiP8P+PFTDEvV/WQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('Class', data=df, palette=colors)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c9973d0-83bd-4b09-860e-c1f507f88310",
    "_uuid": "6894af2afdbfd5cd670d00b66f10ae49f1cab421"
   },
   "source": [
    "**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets understand how logistic regression will give results for this kind of dataset. Lets train and test for Credit Default prediction using Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[56864     0]\n",
      " [   98     0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.00      0.00      0.00        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.50      0.50      0.50     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df[['Time', 'Amount']]  # Features\n",
    "y = df['Class']              # Target variable (0 or 1)\n",
    "\n",
    "# Split the data into training and testing sets (adjust test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable on the test data\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "# Calculate and display classification report (includes precision, recall, F1-score)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unreliable results on Imbalanced Dataset: \n",
    "Now this is the problem associated with applcation of Logistic Regression on Imbalanced Datasets. This model is very good from accuracy and precision point of view but it is practically impossible for a dataset to say that There could be No Credit default that there is 0 chances of a Credit Fraud happening based on the given dataset. In practicality this result or inference is not possible because 0.17% of the datavalues are itself indicating towards a default. Hence, we cannot rely on these results and hence there should be correction of the dataset either via Undersampling or Over-Sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "cee315f2-325f-42b6-a640-736f10c272cc",
    "_kg_hide-input": true,
    "_uuid": "cfa51792bf6f8a6b318ae1bffcff4e922b1d1917"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = df['Amount'].values\n",
    "time_val = df['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "72fdda5e-7f82-488d-a433-6157d6180bb8",
    "_uuid": "c5d6781e61c0ee84e72d26e8465bfd98ef91f3b9"
   },
   "source": [
    "<h2> Scaling and Distributing </h2>\n",
    "<a id=\"distributing\"></a>\n",
    "In this phase of our kernel, we will first scale the columns comprise of <b>Time</b> and <b>Amount </b>. Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
    "\n",
    "<h3> What is a sub-Sample?</h3>\n",
    "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
    "\n",
    "<h3> Why do we create a sub-Sample?</h3>\n",
    "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe  will cause the following issues:\n",
    "<ul>\n",
    "<li><b>Overfitting: </b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. </li>\n",
    "<li><b>Wrong Correlations:</b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. </li>\n",
    "</ul>\n",
    "\n",
    "<h3>Summary: </h3> \n",
    "<ul>\n",
    "<li> <b>Scaled amount </b> and <b> scaled time </b> are the columns with scaled values. </li>\n",
    "<li> There are <b>492 cases </b> of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe. </li>\n",
    "<li>We concat the 492 cases of fraud and non fraud, <b>creating a new sub-sample. </b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[30402 26462]\n",
      " [   39    59]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.70     56864\n",
      "           1       0.00      0.60      0.00        98\n",
      "\n",
      "    accuracy                           0.53     56962\n",
      "   macro avg       0.50      0.57      0.35     56962\n",
      "weighted avg       1.00      0.53      0.70     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df[['Time', 'Amount']]  # Features\n",
    "y = df['Class']              # Target variable (0 or 1)\n",
    "\n",
    "# Split the data into training and testing sets (adjust test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Perform random undersampling\n",
    "# Separate majority and minority classes\n",
    "majority_class = X_train[y_train == 0]\n",
    "minority_class = X_train[y_train == 1]\n",
    "\n",
    "# Downsample the majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                replace=False,   # Sample without replacement\n",
    "                                n_samples=len(minority_class),   # Match minority class size\n",
    "                                random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine the minority class with the downsampled majority class\n",
    "X_resampled = np.vstack([majority_downsampled, minority_class])\n",
    "y_resampled = np.concatenate([np.zeros(len(minority_class)), np.ones(len(minority_class))])\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Fit the model on the resampled training data\n",
    "logistic_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict the target variable on the test data\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "# Calculate and display classification report (includes precision, recall, F1-score)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of results: \n",
    "\n",
    "Results with Undersampling:\n",
    "Confusion Matrix:\n",
    "\n",
    "True Negatives (TN): 30,402 <br>\n",
    "False Positives (FP): 26,462<br>\n",
    "False Negatives (FN): 39<br>\n",
    "True Positives (TP): 59<br>\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "Accuracy: 53% <br>\n",
    "Precision for class 1: 0.00 (Very low precision for predicting class 1)<br>\n",
    "Recall for class 1: 0.60 (Higher recall indicates that more actual positives were correctly predicted)<br>\n",
    "F1-Score for class 1: 0.00 (Low F1-score due to low precision)<br>\n",
    "\n",
    "Comparison with Previous Results (Untreated Imbalanced Data):\n",
    "Confusion Matrix:\n",
    "\n",
    "True Negatives (TN): 56,864<br>\n",
    "False Positives (FP): 0<br>\n",
    "False Negatives (FN): 98<br>\n",
    "True Positives (TP): 0<br>\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "Accuracy: 100%<br>\n",
    "Precision for class 1: 0.00 (No true positives, hence precision is zero)<br>\n",
    "Recall for class 1: 0.00 (No true positives, hence recall is zero)<br>\n",
    "F1-Score for class 1: 0.00 (F1-score is zero due to zero precision and recall)<br>\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "1. The undersampling technique has significantly improved the model's ability to predict credit defaults. It has achieved a balance between true positives (detecting actual defaults) and true negatives (correctly identifying non-defaults).\n",
    "2. Although the overall accuracy has decreased, it is now a more reliable measure of the model's performance, as it no longer heavily favors the majority class.\n",
    "3. The model now demonstrates the capability to identify a number of true positives, which is a substantial improvement compared to the previous model.\n",
    "4. However, precision for predicting defaults is still quite low, indicating a high number of false positives. This is an area where further improvement is needed.\n",
    "\n",
    "# This has worked well to some capacity but the accuracy is very lowmaking it unfit for real world use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE (Synthetic Minority Over-sampling Technique): A Brief Overview\n",
    "\n",
    "1. Why is SMOTE Used?\n",
    "\n",
    "Imbalanced Data: In many real-world datasets, one class (usually the minority class) is underrepresented compared to the other(s). This can lead to biased models that favor the majority class.\n",
    "\n",
    "Addressing Imbalance: SMOTE is used to address class imbalance by generating synthetic samples for the minority class, thus making the dataset more balanced.\n",
    "\n",
    "2. Underlying Principles:\n",
    "\n",
    "Local Interpolation: SMOTE works by creating synthetic samples that are similar to the existing minority class samples. It does this by selecting a random instance from the minority class and its k nearest neighbors.\n",
    "\n",
    "Feature Space Expansion: SMOTE then selects random points on the line segments joining these neighbors and creates synthetic samples at these points, effectively expanding the feature space.\n",
    "\n",
    "3. How SMOTE is Applied:\n",
    "\n",
    "Select Target Instance: Randomly choose an instance from the minority class. <br>\n",
    "Find Nearest Neighbors: Identify its k nearest neighbors (commonly set to 5).<br>\n",
    "Random Point Selection: Randomly select one of the k neighbors and call it the \"nearest neighbor.\"<br>\n",
    "Create Synthetic Instance: Generate a random number between 0 and 1. Multiply this number by the difference between the feature values of the target instance and the nearest neighbor, then add it to the feature values of the nearest neighbor. This creates a synthetic instance in the feature space.<br>\n",
    "\n",
    "4. Interpretation:\n",
    "\n",
    "SMOTE augments the dataset with synthetic samples that lie on the line segments joining real instances.\n",
    "\n",
    "These synthetic samples help the model understand the underlying distribution of the minority class better, leading to better generalization.\n",
    "\n",
    "5. Benefits of Using SMOTE:\n",
    "\n",
    "Improved Model Performance: Balancing the dataset often leads to more accurate models, especially for algorithms sensitive to class distribution.<br>\n",
    "Reduced Bias: Helps mitigate the issue of class imbalance, reducing the likelihood of the model favoring the majority class.<br>\n",
    "Preservation of Information: Unlike simple oversampling by duplication, SMOTE generates new samples that expand the feature space, preserving information.<br>\n",
    "Avoiding Overfitting: SMOTE helps prevent overfitting that might occur if the model is trained on an imbalanced dataset.<br>\n",
    "\n",
    "Note: While SMOTE is a powerful technique, it's important to use it judiciously. Sometimes, over-reliance on SMOTE can lead to overfitting or introduce noise. It's advisable to evaluate the impact of SMOTE on model performance through cross-validation and to consider other techniques like adjusting class weights or using different algorithms.\n",
    "\n",
    "Now we will apply SMOTE on the given dataset to balance teh classes and study the change in results for LOgistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 227451, 1: 394})\n",
      "Balanced class distribution after SMOTE: Counter({0: 227451, 1: 227451})\n",
      "Confusion Matrix:\n",
      "[[28529 28335]\n",
      " [   40    58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67     56864\n",
      "           1       0.00      0.59      0.00        98\n",
      "\n",
      "    accuracy                           0.50     56962\n",
      "   macro avg       0.50      0.55      0.34     56962\n",
      "weighted avg       1.00      0.50      0.67     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming df is loaded earlier\n",
    "# (If not, load the dataset before proceeding)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df[['Time', 'Amount']]  # Features\n",
    "y = df['Class']              # Target variable (0 or 1)\n",
    "\n",
    "# Split the data into training and testing sets (adjust test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the balanced sample set constructed for both classes\n",
    "print(f\"Original class distribution: {Counter(y_train)}\")\n",
    "print(f\"Balanced class distribution after SMOTE: {Counter(y_resampled)}\")\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Fit the model on the resampled training data\n",
    "logistic_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict the target variable on the test data\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "# Calculate and display classification report (includes precision, recall, F1-score)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of the results after application of SMOTE: \n",
    "\n",
    "Original Class Distribution: <br>\n",
    "\n",
    "Initially, the dataset was heavily imbalanced. There were 227,451 samples in Class 0 (non-fraudulent transactions) and only 394 samples in Class 1 (fraudulent transactions). This severe class imbalance can lead to biased model predictions.\n",
    "\n",
    "Balanced Class Distribution after SMOTE:\n",
    "\n",
    "After applying SMOTE, the class distribution was transformed. Both Class 0 and Class 1 now have an equal number of samples, each totaling 227,451. This artificial oversampling of the minority class (Class 1) helps to mitigate the effects of class imbalance.\n",
    "\n",
    "Model Performance:<br>\n",
    "\n",
    "Confusion Matrix:<br>\n",
    "\n",
    "The confusion matrix provides a detailed breakdown of the model's predictions. It shows how many samples were correctly or incorrectly classified for each class.\n",
    "\n",
    "In this case, we have:\n",
    "\n",
    "True Negatives (TN): 28,529 - Correctly predicted non-fraudulent transactions.<br>\n",
    "False Positives (FP): 28,335 - Incorrectly predicted fraudulent transactions (Type I error).<br>\n",
    "False Negatives (FN): 40 - Incorrectly predicted non-fraudulent transactions as fraudulent (Type II error).<br>\n",
    "True Positives (TP): 58 - Correctly predicted fraudulent transactions.<br>\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "The classification report provides a comprehensive overview of the model's performance metrics, including precision, recall, F1-score, and support (number of samples).\n",
    "\n",
    "For Class 0 (non-fraudulent transactions):\n",
    "\n",
    "Precision: 1.00 - The proportion of true positives among all predicted positives.<br>\n",
    "Recall: 0.50 - The proportion of true positives among all actual positives.<br>\n",
    "F1-Score: 0.67 - The harmonic mean of precision and recall.<br>\n",
    "Support: 56,864 - Number of samples in this class.<br>\n",
    "\n",
    "For Class 1 (fraudulent transactions):\n",
    "\n",
    "Precision: 0.00 - Due to a very low number of true positives, precision is zero.<br>\n",
    "Recall: 0.59 - A substantial portion of actual positives were correctly predicted.<br>\n",
    "F1-Score: 0.00 - F1-Score is low due to the low precision.<br>\n",
    "Support: 98 - Number of samples in this class.<br>\n",
    "\n",
    "Interpretation:\n",
    "1. Improvement through SMOTE:\n",
    "\n",
    "SMOTE has significantly improved the model's ability to detect fraudulent transactions. It has achieved a balance between identifying true positives for both classes.\n",
    "\n",
    "2. Trade-off between Precision and Recall:\n",
    "\n",
    "While recall for Class 1 has increased, indicating that more true positives are being identified, it comes at the expense of precision. The model is also predicting more false positives.\n",
    "\n",
    "3. Consideration for Real-world Applications:\n",
    "\n",
    "In real-world scenarios, the choice between precision and recall depends on the costs associated with false positives and false negatives. A higher recall is crucial for detecting fraud, even if it leads to some false alarms.\n",
    "\n",
    "4. Further Refinement:\n",
    "\n",
    "Despite the improvement, there is room for further model refinement or exploration of additional techniques to reduce false positives for fraudulent transactions.\n",
    "\n",
    "# Although after application of SMOTE the model is able to predict much better than the case of untreated imbalanced dataset, but here the accuracy is very low and hence it cannot be used in real world. Hence further imporvemnet in the Model is a must. But for the scope of the project we will end the discussion here only to limit to applucation of Logistic Regression in two possible cases of Balanced and Imbalanced Classes. \n",
    "\n",
    "Overall, the application of SMOTE has made the model more reliable in identifying fraudulent transactions. This is crucial for real-world applications where such events are rare but essential to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee0932c1-e9dc-4135-93af-1972d07d3d0f",
    "_uuid": "1b03ca4b3e37985bea49686abd466fdd9a7d84d3"
   },
   "source": [
    "### Conclusion: \n",
    "Implementing SMOTE on our imbalanced dataset helped us with the imbalance of our labels (more no fraud than fraud transactions). Nevertheless, I still have to state that sometimes the neural network on the oversampled dataset predicts less correct fraud transactions than our model using the undersample dataset. However, remember that the removal of outliers was implemented only on the random undersample dataset and not on the oversampled one. Also, in our undersample data our model is unable to detect for a large number of cases non fraud transactions correctly and instead, misclassifies those non fraud transactions as fraud cases. Imagine that people that were making regular purchases got their card blocked due to the reason that our model classified that transaction as a fraud transaction, this will be a huge disadvantage for the financial institution. The number of customer complaints and customer disatisfaction will increase.  The next step of this analysis will be to do an outlier removal on our oversample dataset and see if our accuracy in the test set improves. <br><br>\n",
    "\n",
    "**Note:** One last thing, predictions and accuracies may be subjected to change since I implemented data shuffling on both types of dataframes. The main thing is to see if our models are able to correctly classify no fraud and fraud transactions. I will bring more updates, stay tuned!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
